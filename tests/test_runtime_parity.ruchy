// test_runtime_parity.ruchy â€” Cross-runtime parity tests
//
// Verifies that apr and llama.cpp produce identical text output when running
// the same GGUF model with deterministic greedy decoding.

import std::fs
import std::process

// --- Helpers (ruchy has no file imports, must be inline) ---

fun run_apr(args) {
    let (stdout, stderr, code) = process::execute("apr", args)?
    if code != 0 { return Err(f"apr failed ({code}): {stderr}") }
    Ok(stdout)
}

fun apr_run_json(model_path, prompt) {
    let stdout = run_apr(["run", model_path, "-p", prompt, "-n", "32", "--json"])?
    Ok(parse_json(stdout))
}

fun load_oracle(slug, prompt_name) {
    let text = fs::read_to_string(f"oracle/{slug}/{prompt_name}.json")?
    Ok(parse_json(text))
}

fun is_available(bin) {
    let (_, _, code) = process::execute("which", [bin])?
    Ok(code == 0)
}

fun run_llama_cli(model_path, prompt) {
    let (stdout, stderr, code) = process::execute("llama-cli", [
        "-m", model_path,
        "-p", prompt,
        "-n", "32",
        "--temp", "0",
        "--no-display-prompt",
    ])?
    if code != 0 { return Err(f"llama-cli failed ({code}): {stderr}") }
    Ok(stdout.trim())
}

// --- SmolLM-135M runtime parity ---

@test
fun test_runtime_parity_smollm_arithmetic() {
    if !is_available("llama-cli")? {
        // Skip: llama-cli not installed
        return Ok(())
    }

    let oracle = load_oracle("smollm-135m", "arithmetic")?
    let apr_result = apr_run_json("models/smollm-135m-q4k.gguf", oracle.prompt)?
    let llama_text = run_llama_cli("models/smollm-135m-q4k.gguf", oracle.prompt)?

    assert_eq(apr_result.text.trim(), llama_text,
        f"SmolLM arithmetic: apr '{apr_result.text.trim()}' != llama-cli '{llama_text}'")
}

@test
fun test_runtime_parity_smollm_completion() {
    if !is_available("llama-cli")? {
        return Ok(())
    }

    let oracle = load_oracle("smollm-135m", "completion")?
    let apr_result = apr_run_json("models/smollm-135m-q4k.gguf", oracle.prompt)?
    let llama_text = run_llama_cli("models/smollm-135m-q4k.gguf", oracle.prompt)?

    assert_eq(apr_result.text.trim(), llama_text,
        f"SmolLM completion: apr '{apr_result.text.trim()}' != llama-cli '{llama_text}'")
}

@test
fun test_runtime_parity_smollm_code() {
    if !is_available("llama-cli")? {
        return Ok(())
    }

    let oracle = load_oracle("smollm-135m", "code")?
    let apr_result = apr_run_json("models/smollm-135m-q4k.gguf", oracle.prompt)?
    let llama_text = run_llama_cli("models/smollm-135m-q4k.gguf", oracle.prompt)?

    assert_eq(apr_result.text.trim(), llama_text,
        f"SmolLM code: apr '{apr_result.text.trim()}' != llama-cli '{llama_text}'")
}

@test
fun test_runtime_parity_smollm_greeting() {
    if !is_available("llama-cli")? {
        return Ok(())
    }

    let oracle = load_oracle("smollm-135m", "greeting")?
    let apr_result = apr_run_json("models/smollm-135m-q4k.gguf", oracle.prompt)?
    let llama_text = run_llama_cli("models/smollm-135m-q4k.gguf", oracle.prompt)?

    assert_eq(apr_result.text.trim(), llama_text,
        f"SmolLM greeting: apr '{apr_result.text.trim()}' != llama-cli '{llama_text}'")
}

// --- Qwen2-0.5B runtime parity ---

@test
fun test_runtime_parity_qwen2_arithmetic() {
    if !is_available("llama-cli")? {
        return Ok(())
    }

    let oracle = load_oracle("qwen2-0.5b", "arithmetic")?
    let apr_result = apr_run_json("models/qwen2-0.5b-q4k.gguf", oracle.prompt)?
    let llama_text = run_llama_cli("models/qwen2-0.5b-q4k.gguf", oracle.prompt)?

    assert_eq(apr_result.text.trim(), llama_text,
        f"Qwen2 arithmetic: apr '{apr_result.text.trim()}' != llama-cli '{llama_text}'")
}

@test
fun test_runtime_parity_qwen2_completion() {
    if !is_available("llama-cli")? {
        return Ok(())
    }

    let oracle = load_oracle("qwen2-0.5b", "completion")?
    let apr_result = apr_run_json("models/qwen2-0.5b-q4k.gguf", oracle.prompt)?
    let llama_text = run_llama_cli("models/qwen2-0.5b-q4k.gguf", oracle.prompt)?

    assert_eq(apr_result.text.trim(), llama_text,
        f"Qwen2 completion: apr '{apr_result.text.trim()}' != llama-cli '{llama_text}'")
}

@test
fun test_runtime_parity_qwen2_code() {
    if !is_available("llama-cli")? {
        return Ok(())
    }

    let oracle = load_oracle("qwen2-0.5b", "code")?
    let apr_result = apr_run_json("models/qwen2-0.5b-q4k.gguf", oracle.prompt)?
    let llama_text = run_llama_cli("models/qwen2-0.5b-q4k.gguf", oracle.prompt)?

    assert_eq(apr_result.text.trim(), llama_text,
        f"Qwen2 code: apr '{apr_result.text.trim()}' != llama-cli '{llama_text}'")
}

@test
fun test_runtime_parity_qwen2_greeting() {
    if !is_available("llama-cli")? {
        return Ok(())
    }

    let oracle = load_oracle("qwen2-0.5b", "greeting")?
    let apr_result = apr_run_json("models/qwen2-0.5b-q4k.gguf", oracle.prompt)?
    let llama_text = run_llama_cli("models/qwen2-0.5b-q4k.gguf", oracle.prompt)?

    assert_eq(apr_result.text.trim(), llama_text,
        f"Qwen2 greeting: apr '{apr_result.text.trim()}' != llama-cli '{llama_text}'")
}

// --- GPT-2 runtime parity ---

@test
fun test_runtime_parity_gpt2_arithmetic() {
    if !is_available("llama-cli")? {
        return Ok(())
    }

    let oracle = load_oracle("gpt2-124m", "arithmetic")?
    let apr_result = apr_run_json("models/gpt2-124m-q4k.gguf", oracle.prompt)?
    let llama_text = run_llama_cli("models/gpt2-124m-q4k.gguf", oracle.prompt)?

    assert_eq(apr_result.text.trim(), llama_text,
        f"GPT-2 arithmetic: apr '{apr_result.text.trim()}' != llama-cli '{llama_text}'")
}

@test
fun test_runtime_parity_gpt2_completion() {
    if !is_available("llama-cli")? {
        return Ok(())
    }

    let oracle = load_oracle("gpt2-124m", "completion")?
    let apr_result = apr_run_json("models/gpt2-124m-q4k.gguf", oracle.prompt)?
    let llama_text = run_llama_cli("models/gpt2-124m-q4k.gguf", oracle.prompt)?

    assert_eq(apr_result.text.trim(), llama_text,
        f"GPT-2 completion: apr '{apr_result.text.trim()}' != llama-cli '{llama_text}'")
}

@test
fun test_runtime_parity_gpt2_code() {
    if !is_available("llama-cli")? {
        return Ok(())
    }

    let oracle = load_oracle("gpt2-124m", "code")?
    let apr_result = apr_run_json("models/gpt2-124m-q4k.gguf", oracle.prompt)?
    let llama_text = run_llama_cli("models/gpt2-124m-q4k.gguf", oracle.prompt)?

    assert_eq(apr_result.text.trim(), llama_text,
        f"GPT-2 code: apr '{apr_result.text.trim()}' != llama-cli '{llama_text}'")
}

@test
fun test_runtime_parity_gpt2_greeting() {
    if !is_available("llama-cli")? {
        return Ok(())
    }

    let oracle = load_oracle("gpt2-124m", "greeting")?
    let apr_result = apr_run_json("models/gpt2-124m-q4k.gguf", oracle.prompt)?
    let llama_text = run_llama_cli("models/gpt2-124m-q4k.gguf", oracle.prompt)?

    assert_eq(apr_result.text.trim(), llama_text,
        f"GPT-2 greeting: apr '{apr_result.text.trim()}' != llama-cli '{llama_text}'")
}
